# Python Documentation Assistant

## ğŸ Technical Documentation Assistant using RAG and Prompt Engineering

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Status: Active](https://img.shields.io/badge/status-active-success.svg)](https://github.com/Venkata-Nikhil-Amirisetty/Prompt-Final-Project)

A web-based chatbot that answers questions about Python documentation by retrieving relevant content and generating helpful responses using Google's Gemini API, LangChain, and ChromaDB.

---

## ğŸ“‹ Table of Contents

- [Project Description](#project-description)
- [Architecture Overview](#architecture-overview)
- [Core Components](#core-components)
- [Technologies Used](#technologies-used)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [API Configuration](#api-configuration)
- [Testing](#testing)
- [Performance Metrics](#performance-metrics)
- [Known Limitations](#known-limitations)
- [Future Improvements](#future-improvements)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgments](#acknowledgments)

---

## ğŸ“– Project Description

This project implements a **Retrieval-Augmented Generation (RAG)** system for answering questions about Python programming. The system scrapes official Python documentation, processes it into searchable chunks, generates embeddings, and stores them in a vector database. When users ask questions, the system retrieves relevant documentation and uses Google's Gemini LLM to generate accurate, context-aware responses.

### Why This Project?

- **Educational Purpose**: Demonstrates RAG architecture and prompt engineering techniques
- **Practical Application**: Provides a useful tool for Python developers
- **Modern Stack**: Uses cutting-edge AI and vector database technologies
- **Production-Ready**: Includes error handling, testing, and documentation

### Key Features

- âœ… Intelligent document retrieval using semantic search
- âœ… Context-aware responses with source attribution
- âœ… Conversation memory for follow-up questions
- âœ… Beautiful, responsive web interface
- âœ… Configurable retrieval and generation parameters
- âœ… Comprehensive error handling and logging
- âœ… Performance benchmarking tools

---

## ğŸ—ï¸ Architecture Overview

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Interface â”‚
â”‚   (Streamlit)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Processingâ”‚
â”‚   (Preprocessing)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Retriever    â”‚â”€â”€â”€â”€â”€â–¶â”‚  ChromaDB    â”‚
â”‚  (Embedding +   â”‚      â”‚ Vector Store â”‚
â”‚   Similarity)   â”‚â—€â”€â”€â”€â”€â”€â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Context Format â”‚
â”‚   (Prompts)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM (Gemini)   â”‚
â”‚  Response Gen   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Response  â”‚
â”‚  + Sources      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Document Collection**: Scraper collects Python documentation pages
2. **Processing**: Documents are chunked into manageable pieces
3. **Embedding**: Chunks are converted to vector embeddings
4. **Storage**: Embeddings stored in ChromaDB vector database
5. **Query Processing**: User queries are embedded and searched
6. **Retrieval**: Most relevant document chunks are retrieved
7. **Generation**: LLM generates response based on retrieved context
8. **Response**: Answer and sources are displayed to user

---

## ğŸ”§ Core Components

### Prompt Engineering

The system uses carefully designed prompts to guide the LLM:

- **System Prompt**: Defines the assistant's role and behavior guidelines
- **QA Prompt Template**: Structures context and question for accurate answers
- **Follow-up Prompt**: Handles conversation history for context continuity
- **No Context Prompt**: Graceful fallback when no relevant docs found

**Example Prompt Structure:**
```
System: You are a helpful Python documentation assistant...
Context: [Retrieved documentation chunks]
Question: How do I create a list?
Instructions: Answer based on context, include examples, cite sources...
```

### RAG Implementation

**Retrieval Strategy:**
- Semantic similarity search using cosine distance
- Relevance filtering (threshold: 0.7)
- Optional MMR (Maximum Marginal Relevance) for diversity
- Top-k retrieval (default: 5 documents)

**Generation Strategy:**
- Context-aware prompt construction
- Conversation history integration
- Source attribution in responses
- Error handling and fallbacks

**How They Work Together:**
1. User query â†’ Embedding â†’ Vector search
2. Retrieved docs â†’ Context formatting â†’ Prompt construction
3. Prompt + History â†’ LLM â†’ Generated response
4. Response + Sources â†’ User interface

---

## ğŸ› ï¸ Technologies Used

| Technology | Version | Purpose |
|------------|---------|---------|
| Python | 3.10+ | Core programming language |
| Streamlit | 1.28+ | Web interface framework |
| LangChain | 0.1+ | RAG pipeline orchestration |
| langchain-google-genai | 0.0.6+ | Gemini API integration |
| ChromaDB | 0.4.18+ | Vector database for embeddings |
| Google Gemini | gemini-1.5-flash | Large language model |
| text-embedding-004 | - | Embedding model |
| sentence-transformers | 2.2+ | Fallback embedding model |
| BeautifulSoup4 | 4.12+ | Web scraping |
| pytest | 7.4+ | Testing framework |

### Why These Technologies?

- **LangChain**: Provides robust RAG abstractions and chain management
- **ChromaDB**: Lightweight, local vector database perfect for this use case
- **Gemini**: Fast, cost-effective LLM with good Python knowledge. The flash model provides quick responses suitable for real-time chat.
- **Streamlit**: Rapid UI development with minimal code.
- **text-embedding-004**: Google's latest embedding model optimized for retrieval tasks.

---

## ğŸ“‹ Prerequisites

- **Python 3.10 or higher**
- **Google Account** with Gemini API access
- **Internet connection** for initial scraping and API calls
- **4GB+ RAM** recommended for embedding generation
- **500MB+ disk space** for vector database and cached data

---

## ğŸš€ Installation

### Step 1: Clone the Repository

```bash
git clone https://github.com/Venkata-Nikhil-Amirisetty/Prompt-Final-Project.git
cd prompt-project
```

### Step 2: Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Configure Environment Variables

Create a `.env` file in the project root:

```bash
cp .env.example .env
```

Edit `.env` and add your Google API key:

```
GOOGLE_API_KEY=your_api_key_here
```

Or use:

```
VERTEX_API_KEY=your_api_key_here
```

**How to get a Google API key:**
1. Go to [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Sign in with your Google account
3. Create a new API key
4. Copy the key to your `.env` file

### Step 5: Run Setup Script

```bash
python setup.py
```

This will:
- Verify your environment configuration
- Create necessary directories
- Scrape Python documentation (15-20 pages)
- Generate embeddings
- Build the vector store
- Run basic tests

**Note**: Initial setup may take 10-15 minutes depending on your internet connection and API rate limits.

---

## ğŸ’» Usage

### Starting the Application

```bash
streamlit run app.py
```

The application will open in your default browser at `http://localhost:8501`

### Using the Interface

1. **Ask Questions**: Type your question in the chat input at the bottom
2. **View Sources**: Click on "Sources" expander to see documentation references
3. **Adjust Settings**: Use sidebar to change temperature, number of sources, etc.
4. **Sample Questions**: Click sample questions in sidebar for quick queries
5. **Clear History**: Use "Clear Chat History" button to start fresh

### Example Queries

- "How do I create a list in Python?"
- "Explain Python decorators with an example"
- "What are the main Python data types?"
- "How do I handle exceptions in Python?"
- "What is the difference between a tuple and a list?"

### Expected Outputs

The system will:
- Generate accurate answers based on Python documentation
- Include code examples when relevant
- Cite source documentation
- Show relevance scores for retrieved documents
- Display response time metrics

---

## ğŸ“ Project Structure

```
prompt-project/
â”œâ”€â”€ src/                          # Core source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper.py               # Documentation scraper
â”‚   â”œâ”€â”€ chunker.py               # Text chunking
â”‚   â”œâ”€â”€ embeddings.py            # Embedding generation
â”‚   â”œâ”€â”€ vector_store.py          # ChromaDB operations
â”‚   â”œâ”€â”€ retriever.py             # Document retrieval
â”‚   â”œâ”€â”€ prompts.py               # Prompt templates
â”‚   â””â”€â”€ chain.py                 # RAG chain implementation
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_scraper.py
â”‚   â”œâ”€â”€ test_retriever.py
â”‚   â”œâ”€â”€ test_chain.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ generate_sample_outputs.py
â”‚   â”œâ”€â”€ export_knowledge_base.py
â”‚   â””â”€â”€ performance_benchmark.py
â”œâ”€â”€ examples/                     # Example files
â”‚   â”œâ”€â”€ sample_queries.txt
â”‚   â””â”€â”€ sample_outputs/
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ index.html               # GitHub Pages website
â”‚   â”œâ”€â”€ styles.css
â”‚   â”œâ”€â”€ script.js
â”‚   â””â”€â”€ report_content.md        # PDF report content
â”œâ”€â”€ data/                         # Scraped documentation (generated)
â”œâ”€â”€ chroma_db/                    # Vector database (generated)
â”œâ”€â”€ cache/                        # Embedding cache (generated)
â”œâ”€â”€ outputs/                      # Generated outputs
â”œâ”€â”€ app.py                        # Streamlit main application
â”œâ”€â”€ setup.py                      # Setup script
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .env.example                  # Environment template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ CONTRIBUTING.md
â””â”€â”€ README.md                     # This file
```

---

## ğŸ”‘ API Configuration

### Getting a Google Gemini API Key

1. Visit [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Sign in with your Google account
3. Click "Create API Key"
4. Copy the generated key
5. Add it to your `.env` file as `GOOGLE_API_KEY=your_key_here` or `VERTEX_API_KEY=your_key_here`

### API Usage and Limits

- **Free Tier**: Limited requests per minute
- **Rate Limiting**: Built-in retry logic handles rate limits
- **Cost**: Check Google's pricing for Gemini API
- **Best Practice**: Cache embeddings to minimize API calls

---

## ğŸ§ª Testing

### Running Tests

```bash
# Run all tests
pytest

# Run specific test file
pytest tests/test_scraper.py

# Run with verbose output
pytest -v

# Run with coverage
pytest --cov=src
```

### Expected Test Results

All tests should pass:
- âœ… Scraper tests (URL fetching, content extraction, metadata)
- âœ… Retriever tests (similarity search, filtering, MMR)
- âœ… Chain tests (response generation, memory, error handling)
- âœ… Integration tests (end-to-end flow)

---

## ğŸ“Š Performance Metrics

### Average Performance

- **Response Time**: 2-5 seconds (depending on query complexity)
- **Retrieval Accuracy**: ~85% relevant documents in top-5 results
- **Embedding Generation**: ~100 chunks per minute
- **Vector Search**: <100ms for similarity search

### Benchmarking

Run performance benchmarks:

```bash
python scripts/performance_benchmark.py
```

This generates a detailed performance report in `outputs/performance_benchmark.json`

---

## âš ï¸ Known Limitations

1. **Documentation Scope**: Only covers Python tutorial documentation (not full stdlib)
2. **Language**: English only
3. **Context Window**: Limited by LLM context window (~8000 tokens for Groq models)
4. **Real-time Updates**: Documentation is static (scraped at setup time)
5. **API Dependency**: Requires active internet connection for LLM calls
6. **Rate Limits**: Subject to Groq API rate limits
7. **Embedding Model**: Uses sentence-transformers (may not be optimal for all queries)

---

## ğŸ”® Future Improvements

- [ ] Multi-language support (translate queries and responses)
- [ ] Additional documentation sources (stdlib, third-party packages)
- [ ] Fine-tuned embedding model for Python-specific content
- [ ] User feedback integration for continuous improvement
- [ ] Advanced caching strategies for faster responses
- [ ] Deployment optimizations (Docker, cloud hosting)
- [ ] Real-time documentation updates
- [ ] Code execution in sandboxed environment
- [ ] Multi-modal support (diagrams, images)
- [ ] Collaborative features (shared knowledge bases)

---

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Quick Contribution Guide

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Add tests for new functionality
5. Ensure all tests pass (`pytest`)
6. Commit your changes (`git commit -m 'Add amazing feature'`)
7. Push to the branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

### Code Style

- Follow PEP 8 style guide
- Use type hints for all functions
- Write comprehensive docstrings
- Add tests for new features

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Python Software Foundation** for excellent documentation
- **LangChain** team for the RAG framework
- **ChromaDB** for the vector database
- **Google** for Gemini API
- **Streamlit** for the web framework
- All open-source contributors whose libraries made this project possible

---

## ğŸ“ Support

For issues, questions, or contributions:
- Open an issue on GitHub
- Check the documentation in `docs/`
- Review example queries in `examples/sample_queries.txt`

---

**Built with â¤ï¸ for Python developers**

*Last updated: 2024*

